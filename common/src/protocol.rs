//! 通信プロトコル定義
//!
//! Node↔Coordinator間の通信メッセージ

use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::net::IpAddr;
use uuid::Uuid;

use crate::types::GpuDeviceInfo;

/// ノード登録リクエスト
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct RegisterRequest {
    /// マシン名
    pub machine_name: String,
    /// IPアドレス
    pub ip_address: IpAddr,
    /// ランタイムバージョン（llama.cpp）
    #[serde(rename = "runtime_version", alias = "runtime_version")]
    pub runtime_version: String,
    /// ランタイムポート番号（推論用）
    #[serde(rename = "runtime_port", alias = "runtime_port")]
    pub runtime_port: u16,
    /// GPU利用可能フラグ
    pub gpu_available: bool,
    /// GPUデバイス情報
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub gpu_devices: Vec<GpuDeviceInfo>,
    /// GPU個数
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub gpu_count: Option<u32>,
    /// GPUモデル名
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub gpu_model: Option<String>,
}

/// ノード登録レスポンス
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct RegisterResponse {
    /// ノードID
    pub node_id: Uuid,
    /// ステータス ("registered" または "updated")
    pub status: RegisterStatus,
    /// ノードAPIポート（OpenAI互換API）
    #[serde(skip_serializing_if = "Option::is_none")]
    pub agent_api_port: Option<u16>,
    /// 自動配布されたモデル名（オプション）
    #[serde(skip_serializing_if = "Option::is_none")]
    pub auto_distributed_model: Option<String>,
    /// ダウンロードタスクID（オプション）
    #[serde(skip_serializing_if = "Option::is_none")]
    pub download_task_id: Option<Uuid>,
    /// エージェントトークン（認証用）
    #[serde(skip_serializing_if = "Option::is_none")]
    pub agent_token: Option<String>,
}

/// 登録ステータス
#[derive(Debug, Clone, Copy, Serialize, Deserialize, PartialEq, Eq)]
#[serde(rename_all = "lowercase")]
pub enum RegisterStatus {
    /// 新規登録
    Registered,
    /// 既存ノード更新
    Updated,
}

/// ヘルスチェックリクエスト
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct HealthCheckRequest {
    /// ノードID
    pub node_id: Uuid,
    /// CPU使用率 (0.0-100.0)
    pub cpu_usage: f32,
    /// メモリ使用率 (0.0-100.0)
    pub memory_usage: f32,
    /// GPU使用率 (0.0-100.0)
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub gpu_usage: Option<f32>,
    /// GPUメモリ使用率 (0.0-100.0)
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub gpu_memory_usage: Option<f32>,
    /// GPUメモリ総容量 (MB)
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub gpu_memory_total_mb: Option<u64>,
    /// GPU使用メモリ (MB)
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub gpu_memory_used_mb: Option<u64>,
    /// GPU温度 (℃)
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub gpu_temperature: Option<f32>,
    /// GPUモデル名
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub gpu_model_name: Option<String>,
    /// GPU計算能力 (例: "8.9")
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub gpu_compute_capability: Option<String>,
    /// GPU能力スコア (0-10000)
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub gpu_capability_score: Option<u32>,
    /// 処理中リクエスト数
    pub active_requests: u32,
    /// 過去N件の平均レスポンスタイム (ms)
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub average_response_time_ms: Option<f32>,
    /// ノードがロード済みのモデル一覧
    #[serde(default)]
    pub loaded_models: Vec<String>,
    /// モデル起動中フラグ
    #[serde(default)]
    pub initializing: bool,
    /// 起動済みモデル数/総数
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub ready_models: Option<(u8, u8)>,
}

/// LLM runtimeチャットリクエスト
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChatRequest {
    /// モデル名
    pub model: String,
    /// メッセージ配列
    pub messages: Vec<ChatMessage>,
    /// ストリーミング有効化
    #[serde(default)]
    pub stream: bool,
}

/// チャットメッセージ
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChatMessage {
    /// ロール ("user", "assistant", "system")
    pub role: String,
    /// メッセージ内容
    pub content: String,
}

/// LLM runtimeチャットレスポンス
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChatResponse {
    /// レスポンスメッセージ
    pub message: ChatMessage,
    /// 完了フラグ
    pub done: bool,
}

/// LLM runtime Generateリクエスト
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GenerateRequest {
    /// モデル名
    pub model: String,
    /// プロンプト
    pub prompt: String,
    /// ストリーミング有効化
    #[serde(default)]
    pub stream: bool,
}

/// リクエスト/レスポンスレコード
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RequestResponseRecord {
    /// レコードの一意識別子
    pub id: Uuid,
    /// リクエスト受信時刻
    pub timestamp: DateTime<Utc>,
    /// リクエストタイプ（Chat または Generate）
    pub request_type: RequestType,
    /// 使用されたモデル名
    pub model: String,
    /// 処理したノードのID
    pub node_id: Uuid,
    /// ノードのマシン名
    pub agent_machine_name: String,
    /// ノードのIPアドレス
    pub agent_ip: IpAddr,
    /// リクエスト元クライアントのIPアドレス
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub client_ip: Option<IpAddr>,
    /// リクエスト本文（JSON形式）
    pub request_body: serde_json::Value,
    /// レスポンス本文（JSON形式、エラー時はNone）
    pub response_body: Option<serde_json::Value>,
    /// 処理時間（ミリ秒）
    pub duration_ms: u64,
    /// レコードのステータス（成功 or エラー）
    pub status: RecordStatus,
    /// レスポンス完了時刻
    pub completed_at: DateTime<Utc>,
}

/// リクエストタイプ
#[derive(Debug, Clone, Copy, Serialize, Deserialize, PartialEq, Eq)]
#[serde(rename_all = "lowercase")]
pub enum RequestType {
    /// /api/chat エンドポイント
    Chat,
    /// /api/generate エンドポイント
    Generate,
    /// /v1/embeddings エンドポイント
    Embeddings,
}

/// レコードステータス
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "type", rename_all = "lowercase")]
pub enum RecordStatus {
    /// 正常に処理完了
    Success,
    /// エラー発生
    Error {
        /// エラーメッセージ
        message: String,
    },
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_register_request_serialization() {
        let request = RegisterRequest {
            machine_name: "test-machine".to_string(),
            ip_address: "192.168.1.100".parse().unwrap(),
            runtime_version: "0.1.0".to_string(),
            runtime_port: 11434,
            gpu_available: true,
            gpu_devices: vec![GpuDeviceInfo {
                model: "NVIDIA RTX 4090".to_string(),
                count: 2,
                memory: None,
            }],
            gpu_count: Some(2),
            gpu_model: Some("NVIDIA RTX 4090".to_string()),
        };

        let json = serde_json::to_string(&request).unwrap();
        let deserialized: RegisterRequest = serde_json::from_str(&json).unwrap();

        assert_eq!(request, deserialized);
    }

    #[test]
    fn test_register_status_serialization() {
        assert_eq!(
            serde_json::to_string(&RegisterStatus::Registered).unwrap(),
            "\"registered\""
        );
        assert_eq!(
            serde_json::to_string(&RegisterStatus::Updated).unwrap(),
            "\"updated\""
        );
    }

    #[test]
    fn test_health_check_request_serialization() {
        let request = HealthCheckRequest {
            node_id: Uuid::new_v4(),
            cpu_usage: 45.5,
            memory_usage: 60.2,
            gpu_usage: Some(33.0),
            gpu_memory_usage: Some(71.0),
            gpu_memory_total_mb: Some(8192),
            gpu_memory_used_mb: Some(5800),
            gpu_temperature: Some(72.5),
            gpu_model_name: None,
            gpu_compute_capability: None,
            gpu_capability_score: None,
            active_requests: 3,
            average_response_time_ms: Some(123.4),
            loaded_models: vec!["gpt-oss:20b".to_string()],
            initializing: true,
            ready_models: Some((1, 2)),
        };

        let json = serde_json::to_string(&request).unwrap();
        let deserialized: HealthCheckRequest = serde_json::from_str(&json).unwrap();

        assert_eq!(request.cpu_usage, deserialized.cpu_usage);
        assert_eq!(request.memory_usage, deserialized.memory_usage);
        assert_eq!(request.gpu_usage, deserialized.gpu_usage);
        assert_eq!(request.gpu_memory_usage, deserialized.gpu_memory_usage);
        assert_eq!(request.active_requests, deserialized.active_requests);
        assert_eq!(
            request.average_response_time_ms,
            deserialized.average_response_time_ms
        );
        assert_eq!(request.loaded_models, deserialized.loaded_models);
    }

    #[test]
    fn test_health_check_request_with_gpu_capability() {
        let request = HealthCheckRequest {
            node_id: Uuid::new_v4(),
            cpu_usage: 50.0,
            memory_usage: 60.0,
            gpu_usage: Some(40.0),
            gpu_memory_usage: Some(50.0),
            gpu_memory_total_mb: Some(16384),
            gpu_memory_used_mb: Some(8192),
            gpu_temperature: Some(65.0),
            gpu_model_name: Some("NVIDIA GeForce RTX 4090".to_string()),
            gpu_compute_capability: Some("8.9".to_string()),
            gpu_capability_score: Some(9500),
            active_requests: 2,
            average_response_time_ms: Some(100.0),
            loaded_models: vec!["llama3:8b".to_string()],
            initializing: false,
            ready_models: Some((1, 1)),
        };

        let json = serde_json::to_string(&request).unwrap();
        let deserialized: HealthCheckRequest = serde_json::from_str(&json).unwrap();

        assert_eq!(request.gpu_model_name, deserialized.gpu_model_name);
        assert_eq!(
            request.gpu_compute_capability,
            deserialized.gpu_compute_capability
        );
        assert_eq!(
            request.gpu_capability_score,
            deserialized.gpu_capability_score
        );
    }

    #[test]
    fn test_chat_request_default_stream_false() {
        let json = r#"{"model":"llama2","messages":[{"role":"user","content":"Hello"}]}"#;
        let request: ChatRequest = serde_json::from_str(json).unwrap();

        assert!(!request.stream);
    }
}
