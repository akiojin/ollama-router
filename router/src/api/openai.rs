//! OpenAI互換APIエンドポイント (/v1/*)

use axum::{
    extract::{Path, State},
    http::{header::CONTENT_TYPE, HeaderValue, StatusCode},
    response::{IntoResponse, Response},
    Json,
};
use chrono::Utc;
use ollama_router_common::{
    error::{CommonError, RouterError},
    protocol::{RecordStatus, RequestResponseRecord, RequestType},
};
use serde_json::{json, Value};
use std::time::Instant;
use axum::body::Body;
use uuid::Uuid;
use reqwest;

use crate::{
    api::{
        agent::AppError,
        proxy::{forward_streaming_response, save_request_record, select_available_agent},
    },
    balancer::RequestOutcome,
    AppState,
};

fn map_reqwest_error(err: reqwest::Error) -> AppError {
    AppError::from(RouterError::Http(err.to_string()))
}

/// POST /v1/chat/completions - OpenAI互換チャットAPI
pub async fn chat_completions(
    State(state): State<AppState>,
    Json(payload): Json<Value>,
) -> Result<Response, AppError> {
    let model = extract_model(&payload)?;
    let stream = extract_stream(&payload);
    proxy_openai_post(
        &state,
        payload,
        "/v1/chat/completions",
        model,
        stream,
        RequestType::Chat,
    )
    .await
}

/// POST /v1/completions - OpenAI互換テキスト補完API
pub async fn completions(
    State(state): State<AppState>,
    Json(payload): Json<Value>,
) -> Result<Response, AppError> {
    let model = extract_model(&payload)?;
    let stream = extract_stream(&payload);
    proxy_openai_post(
        &state,
        payload,
        "/v1/completions",
        model,
        stream,
        RequestType::Generate,
    )
    .await
}

/// POST /v1/embeddings - OpenAI互換Embeddings API
pub async fn embeddings(
    State(state): State<AppState>,
    Json(payload): Json<Value>,
) -> Result<Response, AppError> {
    let model = extract_model(&payload)?;
    proxy_openai_post(
        &state,
        payload,
        "/v1/embeddings",
        model,
        false,
        RequestType::Embeddings,
    )
    .await
}

/// GET /v1/models - モデル一覧取得
pub async fn list_models(State(_state): State<AppState>) -> Result<Response, AppError> {
    // ルーターがサポートするモデルを返す（プロキシせずローカルリストを使用）
    let client = crate::ollama::OllamaClient::new()?;
    let models = client.get_predefined_models();

    // OpenAI互換レスポンス形式に合わせる
    // https://platform.openai.com/docs/api-reference/models/list
    let data: Vec<Value> = models
        .into_iter()
        .map(|m| {
            json!({
                "id": m.name,
                "object": "model",
                "created": 0,
                "owned_by": "coordinator",
            })
        })
        .collect();

    let body = json!({
        "object": "list",
        "data": data,
    });

    Ok((StatusCode::OK, Json(body)).into_response())
}

/// GET /v1/models/:id - モデル詳細取得
pub async fn get_model(
    State(_state): State<AppState>,
    Path(model_id): Path<String>,
) -> Result<Response, AppError> {
    let client = crate::ollama::OllamaClient::new()?;
    let exists = client
        .get_predefined_models()
        .into_iter()
        .any(|m| m.name == model_id);

    if !exists {
        // 404 を OpenAI 換算で返す
        let body = json!({
            "error": {
                "message": "The model does not exist",
                "type": "invalid_request_error",
                "param": "model",
                "code": "model_not_found"
            }
        });
        return Ok((StatusCode::NOT_FOUND, Json(body)).into_response());
    }

    let body = json!({
        "id": model_id,
        "object": "model",
        "created": 0,
        "owned_by": "coordinator",
    });

    Ok((StatusCode::OK, Json(body)).into_response())
}

fn extract_model(payload: &Value) -> Result<String, AppError> {
    payload
        .get("model")
        .and_then(|v| v.as_str())
        .map(|s| s.to_string())
        .ok_or_else(|| validation_error("`model` field is required for OpenAI-compatible requests"))
}

fn extract_stream(payload: &Value) -> bool {
    payload
        .get("stream")
        .and_then(|v| v.as_bool())
        .unwrap_or(false)
}

fn parse_cloud_model(model: &str) -> Option<(String, String)> {
    // Accept prefixes like "openai:foo", "google:bar", "anthropic:baz"
    let prefixes = ["openai:", "google:", "anthropic:", "ahtnorpic:"];
    for p in prefixes.iter() {
        if model.starts_with(p) {
            let rest = model.trim_start_matches(p);
            if rest.is_empty() {
                return None;
            }
            let provider = if *p == "ahtnorpic:" {
                "anthropic"
            } else {
                p.trim_end_matches(':')
            };
            return Some((provider.to_string(), rest.to_string()));
        }
    }
    None
}

async fn proxy_openai_cloud_post(
    target_path: &str,
    model: &str,
    stream: bool,
    payload: Value,
) -> Result<Response, AppError> {
    if stream {
        return Err(validation_error(
            "stream=true is not supported for cloud-prefixed models yet",
        ));
    }

    let (provider, _rest) = parse_cloud_model(model)
        .ok_or_else(|| validation_error("cloud model prefix is invalid"))?;

    match provider.as_str() {
        "openai" => {
            let api_key = std::env::var("OPENAI_API_KEY")
                .map_err(|_| validation_error("OPENAI_API_KEY is required for openai: models"))?;

            let client = reqwest::Client::new();
            let url = format!("https://api.openai.com{target_path}");
            let res = client
                .post(&url)
                .bearer_auth(api_key)
                .json(&payload)
                .send()
                .await
                .map_err(map_reqwest_error)?;

            let status =
                StatusCode::from_u16(res.status().as_u16()).unwrap_or(StatusCode::BAD_GATEWAY);
            let ct = res.headers().get(reqwest::header::CONTENT_TYPE).cloned();
            let bytes = res.bytes().await.map_err(map_reqwest_error)?;
            let mut resp = Response::builder().status(status);
            if let Some(ct) = ct {
                if let Ok(hv) = HeaderValue::from_str(ct.to_str().unwrap_or("")) {
                    resp = resp.header(CONTENT_TYPE, hv);
                }
            }
            Ok(resp.body(Body::from(bytes)).unwrap())
        }
        "google" | "anthropic" => Err(validation_error(
            "google:/anthropic: prefixes are reserved; direct API mapping not yet implemented",
        )),
        _ => Err(validation_error("unsupported cloud provider prefix")),
    }
}

async fn proxy_openai_post(
    state: &AppState,
    payload: Value,
    target_path: &str,
    model: String,
    stream: bool,
    request_type: RequestType,
) -> Result<Response, AppError> {
    // Cloud-prefixed model -> forward to provider API
    if parse_cloud_model(&model).is_some() {
        return proxy_openai_cloud_post(target_path, &model, stream, payload).await;
    }

    let record_id = Uuid::new_v4();
    let timestamp = Utc::now();
    let request_body = payload.clone();

    let agent = select_available_agent(state).await?;
    let node_id = agent.id;
    let agent_machine_name = agent.machine_name.clone();
    let agent_ip = agent.ip_address;

    state
        .load_manager
        .begin_request(node_id)
        .await
        .map_err(AppError::from)?;

    let client = reqwest::Client::new();
    let agent_api_port = agent.ollama_port + 1;
    let ollama_url = format!(
        "http://{}:{}{}",
        agent.ip_address, agent_api_port, target_path
    );
    let start = Instant::now();

    let response = match client.post(&ollama_url).json(&payload).send().await {
        Ok(res) => res,
        Err(e) => {
            let duration = start.elapsed();
            state
                .load_manager
                .finish_request(node_id, RequestOutcome::Error, duration)
                .await
                .map_err(AppError::from)?;

            save_request_record(
                state.request_history.clone(),
                RequestResponseRecord {
                    id: record_id,
                    timestamp,
                    request_type,
                    model: model.clone(),
                    node_id,
                    agent_machine_name: agent_machine_name.clone(),
                    agent_ip,
                    client_ip: None,
                    request_body: request_body.clone(),
                    response_body: None,
                    duration_ms: duration.as_millis() as u64,
                    status: RecordStatus::Error {
                        message: format!("Failed to proxy OpenAI request: {}", e),
                    },
                    completed_at: Utc::now(),
                },
            );

            return Err(RouterError::Http(format!("Failed to proxy OpenAI request: {}", e)).into());
        }
    };

    // ストリームの場合はレスポンスをそのままパススルー
    if stream {
        let duration = start.elapsed();
        state
            .load_manager
            .finish_request(node_id, RequestOutcome::Success, duration)
            .await
            .map_err(AppError::from)?;

        save_request_record(
            state.request_history.clone(),
            RequestResponseRecord {
                id: record_id,
                timestamp,
                request_type,
                model: model.clone(),
                node_id,
                agent_machine_name: agent_machine_name.clone(),
                agent_ip,
                client_ip: None,
                request_body: request_body.clone(),
                response_body: None, // ストリームボディは記録しない
                duration_ms: duration.as_millis() as u64,
                status: RecordStatus::Success,
                completed_at: Utc::now(),
            },
        );

        return forward_streaming_response(response).map_err(AppError::from);
    }

    if !response.status().is_success() {
        let duration = start.elapsed();
        state
            .load_manager
            .finish_request(node_id, RequestOutcome::Error, duration)
            .await
            .map_err(AppError::from)?;

        let status = response.status();
        let status_code = StatusCode::from_u16(status.as_u16()).unwrap_or(StatusCode::BAD_GATEWAY);
        let body_bytes = response.bytes().await.unwrap_or_default();
        let message = if body_bytes.is_empty() {
            status.to_string()
        } else {
            String::from_utf8_lossy(&body_bytes).trim().to_string()
        };

        save_request_record(
            state.request_history.clone(),
            RequestResponseRecord {
                id: record_id,
                timestamp,
                request_type,
                model: model.clone(),
                node_id,
                agent_machine_name: agent_machine_name.clone(),
                agent_ip,
                client_ip: None,
                request_body: request_body.clone(),
                response_body: None,
                duration_ms: duration.as_millis() as u64,
                status: RecordStatus::Error {
                    message: message.clone(),
                },
                completed_at: Utc::now(),
            },
        );

        let payload = json!({
            "error": {
                "message": message,
                "type": "ollama_upstream_error",
                "code": status_code.as_u16(),
            }
        });

        return Ok((status_code, Json(payload)).into_response());
    }

    if stream {
        let duration = start.elapsed();
        state
            .load_manager
            .finish_request(node_id, RequestOutcome::Success, duration)
            .await
            .map_err(AppError::from)?;

        save_request_record(
            state.request_history.clone(),
            RequestResponseRecord {
                id: record_id,
                timestamp,
                request_type,
                model,
                node_id,
                agent_machine_name,
                agent_ip,
                client_ip: None,
                request_body,
                response_body: None,
                duration_ms: duration.as_millis() as u64,
                status: RecordStatus::Success,
                completed_at: Utc::now(),
            },
        );

        return forward_streaming_response(response).map_err(AppError::from);
    }

    let parsed = response.json::<Value>().await;
    let duration = start.elapsed();

    match parsed {
        Ok(body) => {
            state
                .load_manager
                .finish_request(node_id, RequestOutcome::Success, duration)
                .await
                .map_err(AppError::from)?;

            save_request_record(
                state.request_history.clone(),
                RequestResponseRecord {
                    id: record_id,
                    timestamp,
                    request_type,
                    model,
                    node_id,
                    agent_machine_name,
                    agent_ip,
                    client_ip: None,
                    request_body,
                    response_body: Some(body.clone()),
                    duration_ms: duration.as_millis() as u64,
                    status: RecordStatus::Success,
                    completed_at: Utc::now(),
                },
            );

            Ok((StatusCode::OK, Json(body)).into_response())
        }
        Err(e) => {
            state
                .load_manager
                .finish_request(node_id, RequestOutcome::Error, duration)
                .await
                .map_err(AppError::from)?;

            save_request_record(
                state.request_history.clone(),
                RequestResponseRecord {
                    id: record_id,
                    timestamp,
                    request_type,
                    model,
                    node_id,
                    agent_machine_name,
                    agent_ip,
                    client_ip: None,
                    request_body,
                    response_body: None,
                    duration_ms: duration.as_millis() as u64,
                    status: RecordStatus::Error {
                        message: format!("Failed to parse OpenAI response: {}", e),
                    },
                    completed_at: Utc::now(),
                },
            );

            Err(RouterError::Http(format!("Failed to parse OpenAI response: {}", e)).into())
        }
    }
}

#[allow(dead_code)]
async fn proxy_openai_get(state: &AppState, target_path: &str) -> Result<Response, AppError> {
    let agent = select_available_agent(state).await?;
    let node_id = agent.id;

    state
        .load_manager
        .begin_request(node_id)
        .await
        .map_err(AppError::from)?;

    let client = reqwest::Client::new();
    let ollama_url = format!(
        "http://{}:{}{}",
        agent.ip_address, agent.ollama_port, target_path
    );
    let start = Instant::now();

    let response = client.get(&ollama_url).send().await.map_err(|e| {
        AppError::from(RouterError::Http(format!(
            "Failed to proxy OpenAI models request: {}",
            e
        )))
    })?;

    let duration = start.elapsed();
    let outcome = if response.status().is_success() {
        RequestOutcome::Success
    } else {
        RequestOutcome::Error
    };
    state
        .load_manager
        .finish_request(node_id, outcome, duration)
        .await
        .map_err(AppError::from)?;

    if !response.status().is_success() {
        let status = response.status();
        let status_code = StatusCode::from_u16(status.as_u16()).unwrap_or(StatusCode::BAD_GATEWAY);
        let body_bytes = response.bytes().await.unwrap_or_default();
        let message = if body_bytes.is_empty() {
            status.to_string()
        } else {
            String::from_utf8_lossy(&body_bytes).trim().to_string()
        };

        let payload = json!({
            "error": {
                "message": message,
                "type": "ollama_upstream_error",
                "code": status_code.as_u16(),
            }
        });

        return Ok((status_code, Json(payload)).into_response());
    }

    let body = response.json::<Value>().await.map_err(|e| {
        AppError::from(RouterError::Http(format!(
            "Failed to parse OpenAI models response: {}",
            e
        )))
    })?;

    Ok((StatusCode::OK, Json(body)).into_response())
}

fn validation_error(message: impl Into<String>) -> AppError {
    let err = RouterError::Common(CommonError::Validation(message.into()));
    err.into()
}

#[cfg(test)]
mod tests {
    use super::{parse_cloud_model, proxy_openai_cloud_post};
    use serde_json::json;

    #[test]
    fn parse_cloud_prefixes() {
        assert_eq!(
            parse_cloud_model("openai:gpt-4o"),
            Some(("openai".to_string(), "gpt-4o".to_string()))
        );
        assert_eq!(
            parse_cloud_model("google:gemini-pro"),
            Some(("google".to_string(), "gemini-pro".to_string()))
        );
        assert_eq!(
            parse_cloud_model("ahtnorpic:claude-3"),
            Some(("anthropic".to_string(), "claude-3".to_string()))
        );
        assert_eq!(parse_cloud_model("gpt-4"), None);
        assert_eq!(parse_cloud_model("openai:"), None);
    }

    #[tokio::test]
    async fn openai_prefix_requires_api_key() {
        let payload = json!({"model":"openai:gpt-4o","messages":[]});
        let err = proxy_openai_cloud_post("/v1/chat/completions", "openai:gpt-4o", false, payload)
            .await
            .unwrap_err();
        assert!(
            err.to_string().contains("OPENAI_API_KEY"),
            "expected error mentioning OPENAI_API_KEY, got {}",
            err
        );
    }

    #[tokio::test]
    async fn google_prefix_returns_not_implemented() {
        let payload = json!({"model":"google:gemini-pro","messages":[]});
        let err = proxy_openai_cloud_post("/v1/chat/completions", "google:gemini-pro", false, payload)
            .await
            .unwrap_err();
        assert!(
            err.to_string().contains("not yet implemented") || err.to_string().contains("reserved"),
            "expected not implemented error, got {}",
            err
        );
    }
}
