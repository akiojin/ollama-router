services:
  llm-router:
    build:
      context: .
    container_name: llm-router
    mem_limit: 16g
    mem_reservation: 13g
    memswap_limit: 18g
    volumes:
      - .:/llm-router
      - bash-history:/root/.bash_history_dir
      - ./.docker/bash_history:/root/.bash_history
      - claude-config:/root/.claude
      - codex-config:/root/.codex
      - ${HOME:-~}/.codex:/root/.codex-host:ro
      - ~/.serena:/root/.serena
    environment:
      - OPENAI_API_KEY
      - GOOGLE_API_KEY
      - ANTHROPIC_API_KEY
      - NPM_USER
      - NPM_EMAIL
      - NPM_PASS
      - GITHUB_TOKEN
      - GITHUB_PERSONAL_ACCESS_TOKEN
      - GITHUB_USERNAME
      - GIT_USER_EMAIL
      - HISTFILE=/root/.bash_history_dir/.bash_history
      - HISTSIZE=5000
      - HISTFILESIZE=200000
      - MAX_MCP_OUTPUT_TOKENS
      - ENABLE_BACKGROUND_TASKS=1
      - FORCE_AUTO_BACKGROUND_TASKS=1
    extra_hosts:
      - "host.docker.internal:host-gateway"
    ports:
      - "${ROUTER_PORT:-8080}:8080"
      - "8081:8081"
      - "11434-11440:11434-11440"
    stdin_open: true
    tty: true
    working_dir: /llm-router

  llm-node:
    build:
      context: ./node
      dockerfile: Dockerfile
      args:
        CUDA: ${OLLAMA_CPP_CUDA:-cpu} # cpu | cuda
    container_name: llm-node
    depends_on:
      - llm-router
    environment:
      - OLLAMA_ROUTER_URL=http://llm-router:8080
      - OLLAMA_MODELS_DIR=/models
      - OLLAMA_NODE_PORT=11435
      - OLLAMA_ALLOW_NO_GPU=1
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-}
    runtime: ${DOCKER_RUNTIME:-runc} # set to nvidia when using GPU
    volumes:
      - ./node/models:/models
    ports:
      - "11435:11435"
    restart: unless-stopped

volumes:
  bash-history: {}
  claude-config: {}
  codex-config: {}
