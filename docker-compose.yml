services:
  ollama-router:
    build:
      context: .
    container_name: ollama-router
    mem_limit: 16g
    mem_reservation: 13g
    memswap_limit: 18g
    volumes:
      - .:/ollama-router
      - bash-history:/root/.bash_history_dir
      - ./.docker/bash_history:/root/.bash_history
      - claude-config:/root/.claude
      - codex-config:/root/.codex
      - ${HOME:-~}/.codex:/root/.codex-host:ro
      - ~/.serena:/root/.serena
    environment:
      - NPM_USER
      - NPM_EMAIL
      - NPM_PASS
      - GITHUB_TOKEN
      - GITHUB_PERSONAL_ACCESS_TOKEN
      - GITHUB_USERNAME
      - GIT_USER_EMAIL
      - HISTFILE=/root/.bash_history_dir/.bash_history
      - HISTSIZE=5000
      - HISTFILESIZE=200000
      - MAX_MCP_OUTPUT_TOKENS
      - ENABLE_BACKGROUND_TASKS=1
      - FORCE_AUTO_BACKGROUND_TASKS=1
    extra_hosts:
      - "host.docker.internal:host-gateway"
    ports:
      - "${ROUTER_PORT:-8080}:8080"
    stdin_open: true
    tty: true
    working_dir: /ollama-router

  ollama-node-cpp:
    build:
      context: ./ollama-node-cpp
      dockerfile: Dockerfile
      args:
        CUDA: ${OLLAMA_CPP_CUDA:-cpu} # cpu | cuda
    container_name: ollama-node-cpp
    depends_on:
      - ollama-router
    environment:
      - OLLAMA_ROUTER_URL=http://ollama-router:8080
      - OLLAMA_MODELS_DIR=/models
      - OLLAMA_NODE_PORT=11435
      - OLLAMA_ALLOW_NO_GPU=1
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-}
    runtime: ${DOCKER_RUNTIME:-runc} # set to nvidia when using GPU
    volumes:
      - ./ollama-node-cpp/models:/models
    ports:
      - "11435:11435"
    restart: unless-stopped

volumes:
  bash-history: {}
  claude-config: {}
  codex-config: {}
