#include <gtest/gtest.h>
#include <filesystem>
#include <fstream>

#include "core/llama_manager.h"

using namespace ollama_node;
namespace fs = std::filesystem;

class TempModelFile {
public:
    TempModelFile() {
        base = fs::temp_directory_path() / fs::path("llm-XXXXXX");
        std::string tmpl = base.string();
        std::vector<char> buf(tmpl.begin(), tmpl.end());
        buf.push_back('\0');
        char* created = mkdtemp(buf.data());
        base = created ? fs::path(created) : fs::temp_directory_path();
    }
    ~TempModelFile() {
        std::error_code ec;
        fs::remove_all(base, ec);
    }
    fs::path base;
};

TEST(LlamaManagerTest, LoadsExistingModel) {
    TempModelFile tmp;
    fs::path model = tmp.base / "model.gguf";
    fs::create_directories(model.parent_path());
    // Note: This creates an invalid GGUF file (just the magic bytes)
    // llama.cpp will fail to parse it as a valid model
    std::ofstream(model) << "GGUF";

    LlamaManager mgr(tmp.base.string());
    mgr.setGpuLayerSplit(5);
    // Invalid GGUF file will fail to load in real llama.cpp
    // This test verifies the path resolution and error handling
    EXPECT_FALSE(mgr.loadModel("model.gguf"));
    EXPECT_EQ(mgr.loadedCount(), 0u);
}

TEST(LlamaManagerTest, FailsOnMissingModel) {
    TempModelFile tmp;
    LlamaManager mgr(tmp.base.string());
    EXPECT_FALSE(mgr.loadModel("missing.gguf"));
    EXPECT_EQ(mgr.loadedCount(), 0u);
    EXPECT_EQ(mgr.createContext("missing.gguf"), nullptr);
}

TEST(LlamaManagerTest, RejectsUnsupportedExtension) {
    TempModelFile tmp;
    fs::path model = tmp.base / "bad.txt";
    fs::create_directories(model.parent_path());
    std::ofstream(model) << "bad";
    LlamaManager mgr(tmp.base.string());
    EXPECT_FALSE(mgr.loadModel("bad.txt"));
    EXPECT_EQ(mgr.loadedCount(), 0u);
}

TEST(LlamaManagerTest, TracksMemoryUsageOnLoad) {
    TempModelFile tmp;
    fs::path model1 = tmp.base / "m1.gguf";
    fs::path model2 = tmp.base / "m2.gguf";
    fs::create_directories(model1.parent_path());
    // Invalid GGUF files - llama.cpp will reject them
    std::ofstream(model1) << "GGUF";
    std::ofstream(model2) << "GGUF";

    LlamaManager mgr(tmp.base.string());
    EXPECT_EQ(mgr.memoryUsageBytes(), 0u);
    // Invalid files won't load, memory stays at 0
    mgr.loadModel("m1.gguf");
    mgr.loadModel("m2.gguf");
    EXPECT_EQ(mgr.memoryUsageBytes(), 0u);  // No models loaded
}

TEST(LlamaManagerTest, UnloadReducesMemory) {
    TempModelFile tmp;
    fs::path model = tmp.base / "m.gguf";
    fs::create_directories(model.parent_path());
    // Invalid GGUF file - llama.cpp will reject it
    std::ofstream(model) << "GGUF";
    LlamaManager mgr(tmp.base.string());
    // Invalid file won't load
    EXPECT_FALSE(mgr.loadModel("m.gguf"));
    EXPECT_EQ(mgr.memoryUsageBytes(), 0u);
    // Unloading non-existent model returns false
    EXPECT_FALSE(mgr.unloadModel("m.gguf"));
    EXPECT_EQ(mgr.memoryUsageBytes(), 0u);
}
