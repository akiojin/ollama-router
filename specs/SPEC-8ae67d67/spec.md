# 機能仕様書: ルーター主導のモデル自動配布機能

**機能ID**: `SPEC-8ae67d67`
**作成日**: 2025-11-12
**ステータス**: 下書き
**入力**: ユーザー説明: "ルーター主導のモデル自動配布機能"

## ユーザーシナリオ＆テスト *(必須)*

### ユーザーストーリー1 - ノード登録時の自動モデル配布 (優先度: P1)

管理者として、新しいノードを登録したときに、そのノードのGPU能力に応じた適切なサイズのAIモデルが自動的にダウンロードされ、すぐに推論タスクを実行できる状態になることを期待します。

**この優先度の理由**: これは最も基本的かつ重要な機能です。ノードが登録された直後から使用可能な状態になることで、運用開始までの時間を大幅に短縮し、手動設定の手間を排除できます。

**独立テスト**: ノード登録APIを呼び出し、登録完了後にモデルダウンロードが自動的に開始されることを確認することで、この機能を完全にテストできます。

**受け入れシナリオ**:

1. **前提** 16GB以上のGPUメモリを持つ新しいノードがシステムに存在、**実行** ノード登録APIを呼び出す、**結果** 登録完了後、gpt-oss:20bモデルのダウンロードが自動的に開始され、完了する
2. **前提** 8GB以上16GB未満のGPUメモリを持つノードが登録される、**実行** ノード登録APIを呼び出す、**結果** gpt-oss:7bモデルが自動的にダウンロードされる
3. **前提** 4.5GB以上8GB未満のGPUメモリを持つノードが登録される、**実行** ノード登録APIを呼び出す、**結果** gpt-oss:3bモデルが自動的にダウンロードされる
4. **前提** 4.5GB未満のGPUメモリを持つノードが登録される、**実行** ノード登録APIを呼び出す、**結果** gpt-oss:1bモデルが自動的にダウンロードされる
5. **前提** モデルダウンロード中のノード、**実行** ダッシュボードでノード詳細を表示、**結果** ダウンロード進捗（パーセンテージ）がリアルタイムで表示される

---

### ユーザーストーリー2 - ダッシュボードからの手動モデル配布 (優先度: P2)

管理者として、ダッシュボードから特定のノードまたは全ノードに対して、任意のモデルをダウンロードするように指示できることを期待します。これにより、新しいモデルのテストや、特定のタスクに最適化されたモデルの配布が可能になります。

**この優先度の理由**: 自動配布だけでは不十分なケースに対応します。新しいモデルのリリース時や、特定のユースケースに特化したモデルを配布する必要がある場合に、管理者が柔軟に制御できる必要があります。

**独立テスト**: ダッシュボードUIから「モデルをダウンロード」ボタンをクリックし、指定したノードでモデルダウンロードが開始されることを確認することで、この機能をテストできます。

**受け入れシナリオ**:

1. **前提** ダッシュボードでノード一覧を表示中、**実行** 特定のノードを選択し、モデル名を入力して「ダウンロード」ボタンをクリック、**結果** 選択したノードでモデルダウンロードが開始され、進捗が表示される
2. **前提** ダッシュボードでモデル配布画面を表示中、**実行** 「全ノードに配布」オプションを選択してモデル名を指定、**結果** すべてのオンラインノードで同時にモデルダウンロードが開始される
3. **前提** モデルダウンロード中のノード、**実行** ダウンロード進捗を確認、**結果** 各ノードのダウンロード状況（進行中/完了/失敗）が一覧表示される
4. **前提** ノードがオフライン状態、**実行** そのノードにモデルダウンロードを指示、**結果** エラーメッセージが表示され、「ノードがオンラインになったら再試行」オプションが提示される

---

### ユーザーストーリー3 - モデル情報の可視化 (優先度: P3)

管理者として、どのモデルがダウンロード可能で、各ノードに現在どのモデルがインストールされているかを一覧で確認できることを期待します。これにより、システム全体のモデル配布状況を把握し、適切な管理判断を行えます。

**この優先度の理由**: 情報可視化は運用効率を向上させますが、実際のモデル配布機能（P1、P2）が動作していればシステムとしては機能します。優先度は低いですが、長期的な運用には不可欠です。

**独立テスト**: ダッシュボードで「モデル管理」画面を開き、ダウンロード可能なモデル一覧と各ノードのインストール済みモデルが表示されることを確認することで、この機能をテストできます。

**受け入れシナリオ**:

1. **前提** ダッシュボードでモデル管理画面を表示、**実行** 「ダウンロード可能なモデル」タブを選択、**結果** Ollama公式ライブラリから取得したモデル一覧（名前、サイズ、説明）が表示される
2. **前提** ダッシュボードでノード詳細画面を表示、**実行** 「インストール済みモデル」セクションを確認、**結果** そのノードにインストールされているモデルの一覧（名前、サイズ、インストール日時）が表示される
3. **前提** 複数のノードが異なるモデルをインストール済み、**実行** モデル管理画面でノード別のマトリクス表示を選択、**結果** どのノードにどのモデルがインストールされているかが一目で分かる表形式で表示される

---

### エッジケース

- ダウンロード中にノードがオフラインになった場合、どうなるか？
  - ノード側でダウンロードが中断され、次回オンライン時に再開できる
- 既にインストール済みのモデルを再度ダウンロードしようとした場合、どうなるか？
  - システムは警告メッセージを表示し、「スキップ」または「再ダウンロード」の選択肢を提示する
- ノードのディスク容量が不足している場合、どうなるか？
  - ダウンロード前にディスク容量チェックを行い、不足している場合はエラーメッセージを表示する
- Ollama公式ライブラリAPIがアクセス不能な場合、どうなるか？
  - 既存ノードの `ollama list` からモデル一覧を取得し、縮退運転モードで動作する
- 同時に複数のモデルダウンロードが要求された場合、どうなるか？
  - ノード側でキューイングし、順次ダウンロードを実行する（同時ダウンロード数は設定可能）

## 要件 *(必須)*

### 機能要件

- **FR-001**: システムはノード登録時に、そのノードのGPUメモリサイズに基づいて適切なモデルサイズを自動選択する必要がある
- **FR-002**: システムは管理者がダッシュボードから特定のノードに任意のモデルをダウンロードするよう指示できる必要がある
- **FR-003**: システムは管理者が全ノードに一括でモデルを配布できる必要がある
- **FR-004**: システムはモデルダウンロードの進捗（パーセンテージ、ダウンロード速度、残り時間）をリアルタイムで表示する必要がある
- **FR-005**: システムはOllama公式ライブラリからダウンロード可能なモデル一覧を取得して表示する必要がある
- **FR-006**: システムは各ノードのインストール済みモデル一覧を取得して表示する必要がある
- **FR-007**: システムはモデルダウンロード失敗時に詳細なエラーメッセージを表示する必要がある
- **FR-008**: システムはノードがオフライン時のダウンロード要求を適切に処理し、エラーメッセージを表示する必要がある
- **FR-009**: ノード登録完了後、ルーターが対応している全モデルが当該ノードで自動ダウンロード・常駐状態になること（完了判定はルーターで確認可能）
- **FR-009**: システムは既存のノード自律ダウンロード機能（OLLAMA_DEFAULT_MODEL）と独立して動作する必要がある
- **FR-010**: システムはダウンロード前にノードのディスク容量を確認し、不足している場合は警告を表示する必要がある

### 主要エンティティ

- **モデル情報**: Ollamaモデルの名前、サイズ、説明、必要メモリを表す
- **ダウンロードタスク**: ノードID、モデル名、開始時刻、進捗状況、ステータス（進行中/完了/失敗）を含む
- **ノードモデル状態**: 各ノードにインストールされているモデルの一覧と、それぞれのインストール日時、サイズを保持する

---

## スコープ外 *(オプション)*

以下の機能は本仕様のスコープ外とし、将来のバージョンで対応予定:

- モデルの自動更新機能（新しいバージョンがリリースされたときの自動アップグレード）
- モデルのアンインストール機能（ディスク容量削減のための削除）
- カスタムモデルのアップロード・配布（独自にファインチューニングしたモデルの配布）
- モデルダウンロードのスケジューリング（特定時刻に実行）
- ダウンロード帯域幅の制限設定

---

## 技術制約 *(該当する場合)*

- Ollama公式ライブラリAPIへのアクセスにはインターネット接続が必要
- モデルダウンロードはノード側のディスク容量に依存する
- 進捗情報のリアルタイム更新には、ノードとルーター間の安定したネットワーク接続が必要

---

## 前提条件 *(該当する場合)*

この機能は以下を前提とします:

- ノードがGPU情報を含めて正常に登録されている（SPEC-5cd7b614の要件）
- ノード側にOllamaがインストールされている、または自動インストール機能が有効である
- ルーターとノード間の通信が正常に機能している

---

## 依存関係 *(該当する場合)*

この機能は以下に依存します:

- ノード自己登録システム（SPEC-94621a1f）
- GPU必須ノード登録要件（SPEC-5cd7b614）
- ヘルスチェックシステム（SPEC-443acc8c）- ノードのオンライン/オフライン状態の把握に必要
- 管理ダッシュボード（SPEC-712c20cf）- UI操作のベースとして必要

---

## 対応モデル（固定要件）

ルーターが公式にサポートするモデルは次の4件に限定する。

- gpt-oss:20b
- gpt-oss:120b
- gpt-oss-safeguard:20b
- qwen3-coder:30b

**制約**
- 上記以外のモデルは「対応モデル」として扱わない。UIと /v1/models は常にこの固定リストのみ返す。
- ノード登録完了後、対応モデル4件をまとめて自動ダウンロードし常駐させる（手動配布は不要）。
- 旧GPU帯域別モデル（gpt-oss:7b/3b/1b 等）の自動選択は廃止する。

---

## アーキテクチャ要件（通信経路）

- ルーターはノードの「OpenAI互換API（単一ポート、標準は Ollama ポート+1）」のみと通信する。  
  - 登録ヘルスチェック、モデル自動配布指示、推論プロキシ（chat/completions/embeddings）はすべてノードAPI経由。  
  - ルーターがノード内部の Ollama を直接叩かないこと。  
- ノードは受けた OpenAI互換リクエストを内部の Ollama にルーティングし、必要なモデルを ensure/pull してから実行する。  
- 複数 Ollama 実体を持つ場合の振り分けはノード側の責務とし、ルーターは単一 API エンドポイントだけを認識すればよい。  
- `/v1/models` 応答はルーターが定義する対応モデル固定リストを返す（ノード側で揃える）。

### モデル起動完了までの受付制御
- 対応モデル4件がすべて起動・常駐完了するまではノードは `initializing` 状態とし、ロードバランサの選択対象外とする。  
- ルーターからの推論リクエストは、全ノードが `initializing` の場合は 503（モデル起動中）を返し、後続で待機キューへリクエストを積む。  
- ハートビートで `initializing` フラグと `ready_models`（起動済み/総数）を送信する。  
- ノードAPI `/v1/models` でも進捗情報を返せること（任意だが推奨）。

---

## 成功基準 *(必須)*

以下の成功基準を満たす必要があります:

1. ノード登録から5分以内に、適切なサイズのモデルダウンロードが完了し、推論タスクを実行できる状態になる
2. 管理者がダッシュボードから3クリック以内でモデル配布操作を完了できる
3. モデルダウンロードの進捗情報が5秒以内に更新される
4. ダウンロード可能なモデル一覧の取得が10秒以内に完了する
5. 全ノードへの一括配布操作が30秒以内に開始される（各ノードでのダウンロード開始まで）
6. モデルダウンロード成功率が95%以上である（ネットワークエラー等を除く）
7. ノードのインストール済みモデル一覧の取得が3秒以内に完了する
8. システムは同時に10個のモデルダウンロードタスクを処理できる
9. ノード登録完了後、対応モデル4件（gpt-oss:120b/20b、gpt-oss-safeguard:20b、qwen3-coder:30b）が当該ノードで自動ダウンロード・常駐状態になること（ダッシュボードまたは API で確認可能であること）

---

## ⚡ クイックガイドライン

- ✅ ユーザーが「何を」必要とし「なぜ」必要なのかに焦点を当てる
- ❌ 「どのように」実装するかを避ける (技術スタック、API、コード構造なし)
- 👥 ビジネス関係者向けに記述 (開発者向けではない)
